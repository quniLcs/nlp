{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc3e112",
   "metadata": {},
   "source": [
    "# 0. 准备工作\n",
    "\n",
    "数据集来源于Wikipedia，存储于`data.json`，包含10个类别10000个样本。\n",
    "\n",
    "每个样本都是一个字典，包含三个键：`title`、`label`和`text`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f8d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895d56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = []\n",
    "with open(\"data.json\", \"r\") as file:\n",
    "    for line in file:\n",
    "        dat.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332d4fa",
   "metadata": {},
   "source": [
    "# 1.数据预处理\n",
    "\n",
    "`num`是一个字典，其键为类别名，其值为一个字典。\n",
    "\n",
    "该字典包含三个键：`sample`、`sent`、`word`。\n",
    "\n",
    "## 1.计算每个类别的样本数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d44911e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = {}\n",
    "for sample in dat:\n",
    "    if sample['label'] not in num.keys():\n",
    "        num[sample['label']] = {'sample':1, 'sent':0, 'word':0}\n",
    "    else:\n",
    "        num[sample['label']]['sample'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb01786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Film: 3048\n",
      "      Book: 975\n",
      "Politician: 3824\n",
      "    Writer: 837\n",
      "      Food: 137\n",
      "     Actor: 80\n",
      "    Animal: 93\n",
      "  Software: 266\n",
      "    Artist: 520\n",
      "   Disease: 220\n"
     ]
    }
   ],
   "source": [
    "for label in num.keys():\n",
    "    print( '%10s: %d' % (label, num[label]['sample']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de61fcb",
   "metadata": {},
   "source": [
    "可以看到，各个类别的样本数并不平均。\n",
    "\n",
    "## 2.计算每个类别的平均句子数\n",
    "\n",
    "使用`nltk`中的`sent_tokenize`函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1936655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc58c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dat:\n",
    "    num[sample['label']]['sent'] += len(sent_tokenize(sample['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0124c627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Film: 178.62\n",
      "      Book: 205.26\n",
      "Politician: 225.29\n",
      "    Writer: 217.89\n",
      "      Food: 155.43\n",
      "     Actor: 70.95\n",
      "    Animal: 66.81\n",
      "  Software: 202.62\n",
      "    Artist: 185.04\n",
      "   Disease: 349.60\n"
     ]
    }
   ],
   "source": [
    "for label in num.keys():\n",
    "    print('%10s: %.2f' % (label, (num[label]['sent'] / num[label]['sample'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad776dd5",
   "metadata": {},
   "source": [
    "## 3.计算每个类别的平均单词数\n",
    "\n",
    "使用`nltk`中的`word_tokenize`函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b1f7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "968777e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dat:\n",
    "    num[sample['label']]['word'] += len(word_tokenize(sample['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b132c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Film: 4440.53\n",
      "      Book: 5296.66\n",
      "Politician: 5708.66\n",
      "    Writer: 5806.83\n",
      "      Food: 3477.52\n",
      "     Actor: 1719.92\n",
      "    Animal: 1432.11\n",
      "  Software: 4812.89\n",
      "    Artist: 4801.96\n",
      "   Disease: 8012.57\n"
     ]
    }
   ],
   "source": [
    "for label in num.keys():\n",
    "    print('%10s: %.2f' % (label, (num[label]['word'] / num[label]['sample'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afd84e",
   "metadata": {},
   "source": [
    "## 4.数据预处理\n",
    "\n",
    "在`dat`中增加一个键`sent`，存储样本中的单词。\n",
    "\n",
    "对每句话，保留英语单词和数字，去除标点符号和特殊字符。\n",
    "\n",
    "其中，英语单词全都小写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d14f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dat:\n",
    "    sample['sent'] = [[word.lower() for word in word_tokenize(sent) if word.isdigit() or word.isalpha()] \n",
    "                      for sent in sent_tokenize(sample['text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cace4a",
   "metadata": {},
   "source": [
    "## 5.划分数据集\n",
    "\n",
    "训练集包含9000个样本，测试集包含1000个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bcbe5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da2bcb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "index_choice = np.random.choice(10000, 9000, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b93e5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_train = [dat[index_train] for index_train in index_choice]\n",
    "dat_test = [dat[index_test] for index_test in range(10000) if index_test not in index_choice]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f4e69",
   "metadata": {},
   "source": [
    "# 2.建立语言模型\n",
    "## 1.多元模型\n",
    "\n",
    "由于多元模型与样本类型无关，将样本进一步整理为二维列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a95b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_train = [sent for sent in sample['sent'] for sample in dat_train]\n",
    "word_test = [sent for sent in sample['sent'] for sample in dat_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa3ce0",
   "metadata": {},
   "source": [
    "使用`nltk.lm.preprocessing`模块中的`padded_everygram_pipeline`函数，为每句话加上padding，并转化为多元模型。\n",
    "\n",
    "接着，在训练集上使用 Laplace 平滑和参数为 0.1 的 Kneser-Ney 平滑建立一元、二元、三元模型。\n",
    "\n",
    "其中，Laplace 平滑可以使用`nltk.lm`中的`Laplace`类实现，Kneser-Ney 平滑可以使用`nltk.lm`中的`KneserNeyInterpolated`类实现，\n",
    "\n",
    "训练的模型存储于一个三维列表`lm`，每个元素是一个字典，包含两个键：`Laplace`、`Kneser-Ney`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "892c320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import Laplace, KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c2e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = [{},{},{}]\n",
    "for n in (1, 2, 3):\n",
    "    ngrams, vocab = padded_everygram_pipeline(n, word_train)\n",
    "    lm[n - 1]['Laplace'] = Laplace(n)\n",
    "    lm[n - 1]['Laplace'].fit(ngrams, vocab)\n",
    "    \n",
    "    ngrams, vocab = padded_everygram_pipeline(n, word_train)\n",
    "    lm[n - 1]['Kneser-Ney'] = KneserNeyInterpolated(n)\n",
    "    lm[n - 1]['Kneser-Ney'].fit(ngrams, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c9d45",
   "metadata": {},
   "source": [
    "## 2.困惑度\n",
    "\n",
    "在测试集上计算困惑度，其中一元模型的 Kneser-Ney 平滑失效，计算结果为`Inf`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87cde70b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n Laplace Kneser-Ney\n",
      "1 1283.73        Inf\n",
      "2 1047.28      13.26\n",
      "3  947.65       2.83\n"
     ]
    }
   ],
   "source": [
    "print('n', 'Laplace', 'Kneser-Ney')\n",
    "print('%d %7.2f %10s' % (1, lm[0]['Laplace'].perplexity(word_test), 'Inf'))\n",
    "print('%d %7.2f %10.2f' % (2, lm[1]['Laplace'].perplexity(word_test), lm[1]['Kneser-Ney'].perplexity(word_test)))\n",
    "print('%d %7.2f %10.2f' % (3, lm[2]['Laplace'].perplexity(word_test), lm[2]['Kneser-Ney'].perplexity(word_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f25839",
   "metadata": {},
   "source": [
    "可以看到，\n",
    "\n",
    "## 3.造句\n",
    "\n",
    "对于每个模型，造五句话："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bfc251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1:\n",
      "Laplace smoothing:\n",
      "to that indirectly development more in the fever levels of virus mild emergency than partners dengue virus zika the vector\n",
      "away to that develop men is proteins the and a this infections that 12 is spread consider where vaccines a\n",
      "whole which affected and this successfully remains first other other of biosecurity infection in spread zika which number is do\n",
      "countries number human other persists also 2016 this diagnose could zika last this levels pregnant been pregnancies to mothers suspected\n",
      "countries annually in before also in virus the that complete no early brazil apicoargenteus children was there the the case\n",
      "\n",
      "\n",
      "n = 2:\n",
      "Laplace smoothing:\n",
      "there was first detected in february 2016 but is not well as in southeast asia </s> vaccine could potentially spread\n",
      "because zika virus cross the larvicide pyriproxyfen in 2016 </s> the initial zika </s> in singapore after travel with 185\n",
      "zika virus </s> <s> there was not have gone to mothers infected late in singapore after travel is linked to\n",
      "for other flaviviruses it is a 1954 study did not yet </s> the main roles of symptoms of novel technologies\n",
      "for antibodies against the aquatic pools in the typical clinical signs and conjunctivitis and declined through with the vector competence\n",
      "\n",
      "Kneser-Ney smoothing:\n",
      "there was first detected in february 2016 but is not well as in southeast asia </s> uncertain it was the\n",
      "because zika virus cross the larvicide pyriproxyfen in 2016 </s> the initial zika </s> in singapore after travel with 185\n",
      "zika virus </s> active zika virus infections have gone to mothers infected late in singapore after travel is linked to\n",
      "for other flaviviruses it is a 1954 study did not yet </s> the main roles of symptoms of novel technologies\n",
      "for antibodies against the aquatic pools in the typical clinical signs and conjunctivitis and declined through with the vector competence\n",
      "\n",
      "\n",
      "n = 3:\n",
      "Laplace smoothing:\n",
      "the mother initial infection long after the mother initial infection long after the time when zika is easily detected in\n",
      "<s> these abnormalities can lead to dysfunction of the aedes type </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "when returning from travel with or without symptoms it is likely to have born in brazil with microcephaly and miscarriage\n",
      "<s> origin of the primary neural stem cells are to proliferate until the correct number is achieved and then to\n",
      "<s> another line of research considers that zika virus causes microcephaly and chorioretinal scarring </s> </s> </s> </s> </s> </s>\n",
      "\n",
      "Kneser-Ney smoothing:\n",
      "these abnormalities can lead to dysfunction of the mitotic spindle </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "another method which been researched aims to render male mosquitoes that breed with wild female mosquitoes to humans has been\n",
      "whether the stage of pregnancy at which the mother is infected during the first trimester with the virus in french\n",
      "cdc travel advisories included in december 2015 134 were confirmed by pcr or serology and 72 additional cases were reported\n",
      "cdc travel advisories included in december 2020 no active zika outbreaks have been associated with zika virus outbreak </s> </s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('n = 1:')\n",
    "print('Laplace smoothing:')\n",
    "for seed in range(5):\n",
    "    print(' '.join(lm[0]['Laplace'].generate(20, text_seed = ['<s>'], random_seed = seed)))\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('n = 2:')\n",
    "print('Laplace smoothing:')\n",
    "for seed in range(5):\n",
    "    print(' '.join(lm[1]['Laplace'].generate(20, text_seed = ['<s>'], random_seed = seed)))\n",
    "print()\n",
    "print('Kneser-Ney smoothing:')\n",
    "for seed in range(5):\n",
    "    print(' '.join(lm[1]['Kneser-Ney'].generate(20, text_seed = ['<s>'], random_seed = seed)))\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('n = 3:')\n",
    "print('Laplace smoothing:')\n",
    "for seed in range(5):\n",
    "    print(' '.join(lm[2]['Laplace'].generate(20, text_seed = ['<s>'], random_seed = seed)))\n",
    "print()\n",
    "print('Kneser-Ney smoothing:')\n",
    "for seed in range(5):\n",
    "    print(' '.join(lm[2]['Kneser-Ney'].generate(20, text_seed = ['<s>'], random_seed = seed)))\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7030d64",
   "metadata": {},
   "source": [
    "可以看到，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72862e",
   "metadata": {},
   "source": [
    "# 3.建立朴素贝叶斯分类器\n",
    "\n",
    "## 1.建立模型\n",
    "\n",
    "编写特征提取函数`extractor`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a41fb31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(sample, vocab):\n",
    "    feature = dict.fromkeys(vocab, 0)\n",
    "    for sent in sample:\n",
    "        for word in sent:\n",
    "            if word in vocab:\n",
    "                feature[word] += 1\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b648094",
   "metadata": {},
   "source": [
    "使用 Laplace 平滑，在30%、50%、70%、90%的训练集上建立模型并存储于一个字典`nb_model`，包含四个键：`0.3`、`0.5`、`0.7`、`0.9`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da36e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = {}\n",
    "nb_test_feature = {}\n",
    "\n",
    "for percentage in (0.3, 0.5, 0.7, 0.9):\n",
    "    index_choice = np.random.choice(9000, int(9000 * percentage), replace = False)\n",
    "    vocab = set([word for index_train in index_choice for sent in dat[index_train]['sent'] for word in sent])\n",
    "    \n",
    "    nb_train_feature = [[item[1] for item in sorted(extractor(dat_train[index_train]['sent'], vocab).items())] \n",
    "                        for index_train in index_choice]\n",
    "    nb_test_feature[percentage] = [[item(1) for item in sorted(extractor(sample['sent'], vocab).items())] for sample in dat_test]\n",
    "    nb_train_label = [dat_train[index_train]['label'] for index_train in index_choice]\n",
    "    \n",
    "    nb_model[percentage]  = CategoricalNB()\n",
    "    nb_model[percentage].fit(nb_train_feature, nb_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8ef17",
   "metadata": {},
   "source": [
    "## 2.计算微观F1值和宏观F1值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test_label = [sample['label'] for sample in dat_test]\n",
    "print('percentage  micro F1  macro F1')\n",
    "\n",
    "for percentage in (0.3, 0.5, 0.7, 0.9):\n",
    "    nb_test_pred = nb_model[percentage].predict(nb_test_feature[percentage])\n",
    "    micro = f1_score(nb_test_label, nb_test_pred, average = 'micro')\n",
    "    macro = f1_score(nb_test_label, nb_test_pred, average = 'micro')\n",
    "    print('%10.1f %8.2f %8.2f' % (micro, macro))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafa0723",
   "metadata": {},
   "source": [
    "可以看到，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
