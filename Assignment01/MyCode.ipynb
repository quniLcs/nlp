{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc3e112",
   "metadata": {},
   "source": [
    "# 0. 准备工作\n",
    "\n",
    "数据集来源于Wikipedia，存储于`data.json`，包含10个类别10000个样本。\n",
    "\n",
    "每个样本都是一个字典，包含三个键：`title`、`label`和`text`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f8d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895d56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = []\n",
    "with open(\"data.json\", \"r\") as file:\n",
    "    for line in file:\n",
    "        dat.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332d4fa",
   "metadata": {},
   "source": [
    "# 1.数据预处理\n",
    "\n",
    "`num`是一个字典，其键为类别名，其值为一个字典。\n",
    "\n",
    "该字典包含三个键：`sample`、`sent`、`word`。\n",
    "\n",
    "## 1.计算每个类别的样本数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d44911e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = {}\n",
    "for sample in dat:\n",
    "    if sample['label'] not in num.keys():\n",
    "        num[sample['label']] = {'sample':1, 'sent':0, 'word':0}\n",
    "    else:\n",
    "        num[sample['label']]['sample'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb01786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Film: 3048\n",
      "      Book: 975\n",
      "Politician: 3824\n",
      "    Writer: 837\n",
      "      Food: 137\n",
      "     Actor: 80\n",
      "    Animal: 93\n",
      "  Software: 266\n",
      "    Artist: 520\n",
      "   Disease: 220\n"
     ]
    }
   ],
   "source": [
    "for label in num.keys():\n",
    "    print( '%10s: %d' % (label, num[label]['sample']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de61fcb",
   "metadata": {},
   "source": [
    "可以看到，各个类别的样本数并不平均。\n",
    "\n",
    "## 2.计算每个类别的平均句子数\n",
    "\n",
    "使用`nltk`中的`sent_tokenize`函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1936655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bc58c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dat:\n",
    "    num[sample['label']]['sent'] += len(sent_tokenize(sample['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0124c627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Film: 178.62\n",
      "      Book: 205.26\n",
      "Politician: 225.29\n",
      "    Writer: 217.89\n",
      "      Food: 155.43\n",
      "     Actor: 70.95\n",
      "    Animal: 66.81\n",
      "  Software: 202.62\n",
      "    Artist: 185.04\n",
      "   Disease: 349.60\n"
     ]
    }
   ],
   "source": [
    "for label in num.keys():\n",
    "    print('%10s: %.2f' % (label, (num[label]['sent'] / num[label]['sample'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad776dd5",
   "metadata": {},
   "source": [
    "## 3.计算每个类别的平均单词数\n",
    "\n",
    "使用`nltk`中的`word_tokenize`函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b1f7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "968777e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dat:\n",
    "    num[sample['label']]['word'] += len(word_tokenize(sample['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b132c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Film: 4440.53\n",
      "      Book: 5296.66\n",
      "Politician: 5708.66\n",
      "    Writer: 5806.83\n",
      "      Food: 3477.52\n",
      "     Actor: 1719.92\n",
      "    Animal: 1432.11\n",
      "  Software: 4812.89\n",
      "    Artist: 4801.96\n",
      "   Disease: 8012.57\n"
     ]
    }
   ],
   "source": [
    "for label in num.keys():\n",
    "    print('%10s: %.2f' % (label, (num[label]['word'] / num[label]['sample'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afd84e",
   "metadata": {},
   "source": [
    "## 4.数据预处理\n",
    "\n",
    "在`dat`中增加一个键`word`，存储样本中的单词。\n",
    "\n",
    "对每句话，保留英语单词和数字，去除标点符号和特殊字符。\n",
    "\n",
    "其中，英语单词全都小写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23d14f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dat:\n",
    "    sample['word'] = [[word.lower() for word in word_tokenize(sent) if word.isdigit() or word.isalpha()] \n",
    "                      for sent in sent_tokenize(sample['text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cace4a",
   "metadata": {},
   "source": [
    "## 5.划分数据集\n",
    "\n",
    "训练集包含9000个样本，测试集包含1000个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bcbe5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da2bcb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "index_choice = np.random.choice(10000, 9000, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b93e5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_train = [dat[index] for index_train in index_choice]\n",
    "dat_test = [dat[index] for index_test in range(10000) if index_test not in index_choice]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f4e69",
   "metadata": {},
   "source": [
    "# 2.建立语言模型\n",
    "## 1.多元模型\n",
    "\n",
    "由于多元模型与样本类型无关，将样本进一步整理为二维列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a95b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_train = [sent for sent in sample['word'] for sample in dat_train]\n",
    "word_test = [sent for sent in sample['word'] for sample in dat_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa3ce0",
   "metadata": {},
   "source": [
    "使用`nltk.lm.preprocessing`模块中的`padded_everygram_pipeline`函数，为每句话加上padding，并转化为多元模型。\n",
    "\n",
    "接着，在训练集上使用 Laplace 平滑和参数为 0.1 的 Kneser-Ney 平滑建立一元、二元、三元模型。\n",
    "\n",
    "其中，Laplace 平滑可以使用`nltk.lm`中的`Laplace`类实现，Kneser-Ney 平滑可以使用`nltk.lm`中的`KneserNeyInterpolated`类实现，\n",
    "\n",
    "训练的模型存储于一个三维列表`lm`，每个元素是一个字典，包含两个键：`Laplace`、`Kneser-Ney`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "892c320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import Laplace, KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "63c2e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = [{},{},{}]\n",
    "for n in (1, 2, 3):\n",
    "    ngrams, vocab = padded_everygram_pipeline(3, word_train)\n",
    "    lm[n - 1]['Laplace'] = Laplace(n)\n",
    "    lm[n - 1]['Laplace'].fit(ngrams, vocab)\n",
    "    \n",
    "    ngrams, vocab = padded_everygram_pipeline(3, word_train)\n",
    "    lm[n - 1]['Kneser-Ney'] = KneserNeyInterpolated(n)\n",
    "    lm[n - 1]['Kneser-Ney'].fit(ngrams, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c9d45",
   "metadata": {},
   "source": [
    "## 2.困惑度\n",
    "\n",
    "在测试集上计算困惑度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cde70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('n', 'Laplace', 'Kneser-Ney')\n",
    "for n in (1, 2, 3):\n",
    "    print('%d %7.2f %10.2f' % (n, lm[n - 1]['Laplace'].perplexity(word_test), lm[n - 1]['Kneser-Ney'].perplexity(word_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f25839",
   "metadata": {},
   "source": [
    "可以看到，\n",
    "\n",
    "## 3.造句\n",
    "\n",
    "对于每个模型，造五句话："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in (1, 2, 3):\n",
    "    print('n = ', n, ':', sep = '')\n",
    "    for smooth in ['Laplace', 'Kneser-Ney']:\n",
    "        print(smooth, 'smoothing:')\n",
    "        for seed in range(5):\n",
    "            print(' '.join(lm[n - 1][smooth].generate(25, text_seed = ['<s>'], random_seed = seed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7030d64",
   "metadata": {},
   "source": [
    "可以看到，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72862e",
   "metadata": {},
   "source": [
    "# 3.建立朴素贝叶斯分类器\n",
    "\n",
    "## 1.建立模型\n",
    "\n",
    "编写特征提取函数`extractor`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41fb31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(sample, vocab):\n",
    "    feature = dict.fromkeys(vocab, 0)\n",
    "    for word in sample:\n",
    "        if word in vocab:\n",
    "            feature[word] += 1\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b648094",
   "metadata": {},
   "source": [
    "使用 Laplace 平滑，在30%、50%、70%、90%的训练集上建立模型并存储于一个字典`nb_model`，包含四个键：`0.3`、`0.5`、`0.7`、`0.9`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = {}\n",
    "nb_test_feature = {}\n",
    "\n",
    "for percentage in (0.3, 0.5, 0.7, 0.9):\n",
    "    index_choice = np.random.choice(9000, 9000 * percentage, replace = False)\n",
    "    vocab = set([word for index_train in index_choice for word in dat[index_train]['word']])\n",
    "    \n",
    "    nb_train_feature = [[item(1) for item in sorted(extractor(dat_train[index_train]['word'], vocab).items())] \n",
    "                        for index_train in index_choice]\n",
    "    nb_test_feature[percentage] = [[item(1) for item in sorted(extractor(sample['word'], vocab).items())] for sample in dat_test]\n",
    "    nb_train_label = [dat_train[index_train]['label'] for index_train in index_choice]\n",
    "    \n",
    "    nb_model[percentage]  = CategoricalNB()\n",
    "    nb_model[percentage].fit(nb_train_feature, nb_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8ef17",
   "metadata": {},
   "source": [
    "## 2.计算微观F1值和宏观F1值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test_label = [sample['label'] for sample in dat_test]\n",
    "print('percentage  micro F1  macro F1')\n",
    "\n",
    "for percentage in (0.3, 0.5, 0.7, 0.9):\n",
    "    nb_test_pred = nb_model[percentage].predict(nb_test_feature[percentage])\n",
    "    micro = f1_score(nb_test_label, nb_test_pred, average = 'micro')\n",
    "    macro = f1_score(nb_test_label, nb_test_pred, average = 'micro')\n",
    "    print('%10.1f %8.2f %8.2f' % (micro, macro))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafa0723",
   "metadata": {},
   "source": [
    "可以看到，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
