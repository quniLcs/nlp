{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc3e112",
   "metadata": {},
   "source": [
    "# 自然语言处理第一次作业\n",
    "\n",
    "王逸群 19307110397\n",
    "\n",
    "GitHub repo: [quniLcs/nlp/Assignment01](https://github.com/quniLcs/nlp/tree/main/Assignment01)\n",
    "\n",
    "## 0. 准备工作\n",
    "\n",
    "数据集来源于Wikipedia，存储于`data.json`，包含10个类别10000个样本。\n",
    "\n",
    "每个样本都是一个字典，包含三个键：`title`、`label`和`text`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f8d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895d56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = []\n",
    "with open(\"data.json\", \"r\") as file:\n",
    "    for line in file:\n",
    "        dat.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332d4fa",
   "metadata": {},
   "source": [
    "## 1.数据预处理\n",
    "\n",
    "`num`是一个字典，其键为类别名，其值为一个字典。\n",
    "\n",
    "该字典包含三个键：`sample`、`sent`、`word`。\n",
    "\n",
    "### 1.计算每个类别的样本数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d44911e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = {}\n",
    "for sample in dat:\n",
    "    if sample['label'] not in num.keys():\n",
    "        num[sample['label']] = {'sample':1, 'sent':0, 'word':0}\n",
    "    else:\n",
    "        num[sample['label']]['sample'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb01786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Film: 3048\n",
      "      Book: 975\n",
      "Politician: 3824\n",
      "    Writer: 837\n",
      "      Food: 137\n",
      "     Actor: 80\n",
      "    Animal: 93\n",
      "  Software: 266\n",
      "    Artist: 520\n",
      "   Disease: 220\n"
     ]
    }
   ],
   "source": [
    "for label in num.keys():\n",
    "    print( '%10s: %d' % (label, num[label]['sample']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de61fcb",
   "metadata": {},
   "source": [
    "可以看到，各个类别的样本数并不平均。\n",
    "\n",
    "### 2.计算每个类别的平均句子数\n",
    "\n",
    "使用`nltk`中的`sent_tokenize`函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1936655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc58c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dat:\n",
    "    num[sample['label']]['sent'] += len(sent_tokenize(sample['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0124c627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Film: 178.62\n",
      "      Book: 205.26\n",
      "Politician: 225.29\n",
      "    Writer: 217.89\n",
      "      Food: 155.43\n",
      "     Actor: 70.95\n",
      "    Animal: 66.81\n",
      "  Software: 202.62\n",
      "    Artist: 185.04\n",
      "   Disease: 349.60\n"
     ]
    }
   ],
   "source": [
    "for label in num.keys():\n",
    "    print('%10s: %.2f' % (label, (num[label]['sent'] / num[label]['sample'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad776dd5",
   "metadata": {},
   "source": [
    "### 3.计算每个类别的平均单词数\n",
    "\n",
    "使用`nltk`中的`word_tokenize`函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b1f7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "968777e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dat:\n",
    "    num[sample['label']]['word'] += len(word_tokenize(sample['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b132c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Film: 4440.53\n",
      "      Book: 5296.66\n",
      "Politician: 5708.66\n",
      "    Writer: 5806.83\n",
      "      Food: 3477.52\n",
      "     Actor: 1719.92\n",
      "    Animal: 1432.11\n",
      "  Software: 4812.89\n",
      "    Artist: 4801.96\n",
      "   Disease: 8012.57\n"
     ]
    }
   ],
   "source": [
    "for label in num.keys():\n",
    "    print('%10s: %.2f' % (label, (num[label]['word'] / num[label]['sample'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afd84e",
   "metadata": {},
   "source": [
    "### 4.数据预处理\n",
    "\n",
    "在`dat`中增加一个键`sent`，存储样本中的单词。\n",
    "\n",
    "对每句话，保留英语单词和数字，去除标点符号和特殊字符。\n",
    "\n",
    "其中，英语单词全都小写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d14f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dat:\n",
    "    sample['sent'] = [[word.lower() for word in word_tokenize(sent) if word.isdigit() or word.isalpha()] \n",
    "                      for sent in sent_tokenize(sample['text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cace4a",
   "metadata": {},
   "source": [
    "### 5.划分数据集\n",
    "\n",
    "训练集包含9000个样本，测试集包含1000个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bcbe5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da2bcb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "index_choice = np.random.choice(10000, 9000, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b93e5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_train = [dat[index_train] for index_train in index_choice]\n",
    "dat_test = [dat[index_test] for index_test in range(10000) if index_test not in index_choice]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f4e69",
   "metadata": {},
   "source": [
    "## 2.建立语言模型\n",
    "### 1.多元模型\n",
    "\n",
    "由于多元模型与样本类型无关，将样本进一步整理为二维列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a95b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_train = [sent for sample in dat_train for sent in sample['sent'] if sent != []]\n",
    "sent_test = [sent for sample in dat_test for sent in sample['sent'] if sent != []]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa3ce0",
   "metadata": {},
   "source": [
    "使用`nltk.lm.preprocessing`模块中的`padded_everygram_pipeline`函数，为每句话加上padding，并转化为多元模型。\n",
    "\n",
    "接着，在训练集上使用 Laplace 平滑和参数为 0.1 的 Kneser-Ney 平滑建立一元、二元、三元模型。\n",
    "\n",
    "其中，Laplace 平滑可以使用`nltk.lm`中的`Laplace`类实现，Kneser-Ney 平滑可以使用`nltk.lm`中的`KneserNeyInterpolated`类实现，\n",
    "\n",
    "训练的模型存储于一个三维列表`lm`，每个元素是一个字典，包含两个键：`Laplace`、`Kneser-Ney`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "892c320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import Laplace, KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c2e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = [{},{},{}]\n",
    "for n in (1, 2, 3):\n",
    "    ngrams, vocab = padded_everygram_pipeline(n, sent_train)\n",
    "    lm[n - 1]['Laplace'] = Laplace(n)\n",
    "    lm[n - 1]['Laplace'].fit(ngrams, vocab)\n",
    "    \n",
    "    ngrams, vocab = padded_everygram_pipeline(n, sent_train)\n",
    "    lm[n - 1]['Kneser-Ney'] = KneserNeyInterpolated(n)\n",
    "    lm[n - 1]['Kneser-Ney'].fit(ngrams, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd57d8e",
   "metadata": {},
   "source": [
    "Kneser-Ney 平滑计算速度非常慢，以下暂且不考虑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c9d45",
   "metadata": {},
   "source": [
    "### 2.困惑度\n",
    "\n",
    "在测试集上计算困惑度:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87cde70b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n Perplexity\n",
      "1  289890.95\n",
      "2  263961.04\n",
      "3  258824.59\n"
     ]
    }
   ],
   "source": [
    "print('n Perplexity')\n",
    "for n in (1, 2, 3):\n",
    "    print('%d %10.2f' % (n, lm[n - 1]['Laplace'].perplexity(sent_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f25839",
   "metadata": {},
   "source": [
    "可以看到，随着n增大，困惑度减小。\n",
    "\n",
    "### 3.造句\n",
    "\n",
    "对于每个模型，造五句话："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bfc251d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1:\n",
      "the stere in disasters made however that fiction judges of tower local ends states operation der transferred with the to\n",
      "at the subjects did left intellectual polish that and a the in students 1634 inequality sees correct was to a\n",
      "well was again and the siege protocol film on on of belong in his senior year was multiplex individual due\n",
      "curves mumbai he officially ottoman also 2012 the discovers crew years itself the juan pears became participated they mcandrew smith\n",
      "critics and his before also honesty under the such congresses monkey elastic broke ang coming usually the the the careful\n",
      "\n",
      "n = 2:\n",
      "the ring from december 2010 ebert reviewing german including one thing in chicago skyscraper on being told with the torture\n",
      "bringing theatre pedagogy and lindbergh for soong to asuka accepts the house or 16 december 22 2014 wall the agreement\n",
      "von wahlendorf </s> <s> the regiment of hindostan king michael levey and intentional families pay you will in it ended\n",
      "feingold opposed meech lake making a 61 that he does work in the kernel or breaking rocks to mafia money\n",
      "family </s> full confidence </s> german title sound pressure continued to dissolve characterized as an substance use simple sliding doors\n",
      "\n",
      "n = 3:\n",
      "robert reece the film key effects sequences depicting jewish ritual slaughterer </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "<s> the resolution changed senate rules federal law </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "this was a cell to produce politely received novels none of it came out on women with loose organisational structures\n",
      "<s> letters exchanged by the audience </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "<s> at that period </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in (1, 2, 3):\n",
    "    print('n = ', n, ':', sep = '')\n",
    "    for seed in range(5):\n",
    "        print(' '.join(lm[n - 1]['Laplace'].generate(20, text_seed = ['<s>'], random_seed = seed)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7030d64",
   "metadata": {},
   "source": [
    "可以看到，\n",
    "\n",
    "一元模型的连贯性很差，出现了\n",
    "`the to`、\n",
    "`a the in`、\n",
    "`on on of`、\n",
    "`the the the`\n",
    "等尴尬的情况。\n",
    "\n",
    "二元模型的连贯性有所改善，出现了\n",
    "`in chicago`\n",
    "`december 22 2014`\n",
    "`mafia money`\n",
    "`full confidence`\n",
    "`sound pressure`\n",
    "`continued to`\n",
    "`characterized as`\n",
    "`use simple sliding doors`\n",
    "等短语。\n",
    "\n",
    "三元模型的连贯性相对最佳，出现了\n",
    "`key effects`\n",
    "`jewish ritual`\n",
    "`the resolution changed senate rules`\n",
    "`federal law`\n",
    "`this was a cell to produce`\n",
    "`with loose organisational structures`\n",
    "`letters exchanged`\n",
    "`by the audience`\n",
    "`at that period`\n",
    "等短语。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72862e",
   "metadata": {},
   "source": [
    "## 3.建立朴素贝叶斯分类器\n",
    "\n",
    "编写特征提取函数`extractor`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "484b830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a41fb31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(sample, vocab):\n",
    "    feature = OrderedDict.fromkeys(vocab, 0)\n",
    "    for sent in sample:\n",
    "        for word in sent:\n",
    "            if word in vocab:\n",
    "                feature[word] += 1\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b648094",
   "metadata": {},
   "source": [
    "使用 Laplace 平滑，在30%、50%、70%、90%的训练集上建立模型，并计算F1值。\n",
    "\n",
    "由于内存的限制，使用出现频数前3000的词汇作为特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9e33416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61fa3a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage  micro F1  macro F1\n",
      "       0.3      0.93      0.90\n",
      "       0.5      0.94      0.91\n",
      "       0.7      0.91      0.90\n",
      "       0.9      0.93      0.91\n"
     ]
    }
   ],
   "source": [
    "nb_test_label = [sample['label'] for sample in dat_test]\n",
    "print('percentage  micro F1  macro F1')\n",
    "\n",
    "for percentage in (0.3, 0.5, 0.7, 0.9):\n",
    "    index_choice = np.random.choice(9000, int(9000 * percentage), replace = False)\n",
    "    vocab = list(FreqDist([word for index_train in index_choice for sent in dat[index_train]['sent'] for word in sent]).keys())[:3000]\n",
    "    \n",
    "    nb_train_feature = [[value for value in extractor(dat_train[index_train]['sent'], vocab).values()] for index_train in index_choice]\n",
    "    nb_test_feature = [[value for value in extractor(sample['sent'], vocab).values()] for sample in dat_test]\n",
    "    nb_train_label = [dat_train[index_train]['label'] for index_train in index_choice]\n",
    "    \n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(nb_train_feature, nb_train_label)\n",
    "    \n",
    "    nb_test_pred = nb_model.predict(nb_test_feature)\n",
    "    micro = f1_score(nb_test_label, nb_test_pred, average = 'micro')\n",
    "    macro = f1_score(nb_test_label, nb_test_pred, average = 'macro')\n",
    "    print('%10.1f  %8.2f  %8.2f' % (percentage, micro, macro))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafa0723",
   "metadata": {},
   "source": [
    "可以看到，在不同大小的训练集上建立的模型的F1值总体在一个范围内浮动。尽管训练集扩大，一方面改进了模型，另一方面出现了过拟合现象。\n",
    "\n",
    "另外，微观F1值总是大于宏观F1值，是因为它会更依赖于样本数较多的类。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
