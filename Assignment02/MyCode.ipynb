{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6d9b3f",
   "metadata": {},
   "source": [
    "# 自然语言处理第二次作业\n",
    "\n",
    "王逸群 19307110397\n",
    "\n",
    "## 0. 准备工作\n",
    "\n",
    "### 0.1 数据导入\n",
    "\n",
    "数据集来源于Wikipedia，存储于`data.json`，包含10个类别10000个样本。\n",
    "\n",
    "每个样本都是一个字典，包含三个键：`title`、`label`和`text`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2b06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534e5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = []\n",
    "with open(\"data.json\", \"r\") as file:\n",
    "    for line in file:\n",
    "        dat.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d7a25",
   "metadata": {},
   "source": [
    "### 0.2 数据预处理\n",
    "\n",
    "在`dat`中增加两个键，存储样本中的句子和单词。\n",
    "\n",
    "对每句话，保留英语单词和数字，去除标点符号和特殊字符。\n",
    "\n",
    "其中，英语单词全都小写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c346a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10aa93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dat:\n",
    "    sample['sent'] = [[word.lower() for word in word_tokenize(sent) if word.isdigit() or word.isalpha()] \n",
    "                      for sent in sent_tokenize(sample['text'])]\n",
    "    sample['word'] = [word for sent in sample['sent'] for word in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f41c39",
   "metadata": {},
   "source": [
    "另外，生成句子列表和词汇集合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent for sample in dat for sent in sample['sent'] if sent != []]\n",
    "words = set([word for sent in sents for word in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a847c182",
   "metadata": {},
   "source": [
    "## 1. 词汇编码\n",
    "\n",
    "以数据集作为训练集，使用`gensim`工具，生成SGNS词汇编码模型。\n",
    "\n",
    "其中，超参数使用默认参数：\n",
    "\n",
    "编码维度：`vector_size = 100`；\n",
    "\n",
    "窗口大小：`window = 5`；\n",
    "\n",
    "负采样参数：`negative = 5`；\n",
    "\n",
    "初始学习率：`alpha = 0.025`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a15b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3163ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sents, min_count = 1, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecbd0e5",
   "metadata": {},
   "source": [
    "为了方便后续使用，保存模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ad743",
   "metadata": {},
   "source": [
    "后续使用以下代码载入模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e717f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a60c31e",
   "metadata": {},
   "source": [
    "## 2. 词汇相似度\n",
    "\n",
    "随机生成100、1000、10000对词汇，对每个词汇集合，以余弦相似度作为度量，打印5对最相似和5对最不相似的词汇。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7497a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7b80364",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76e2b01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within 100 word pairs, the most similar and unsimilar words:\n",
      "words\t\t\t\t\tdistance\n",
      "wistrom             bly                  0.67645\n",
      "darwaza             lombardi             0.64596\n",
      "athmanathan         kindliness           0.52732\n",
      "kinase              vada                 0.73586\n",
      "gori                zelimkhan            0.61509\n",
      "kaiciid             trickiest           -0.15902\n",
      "kamaras             kallikak            -0.13632\n",
      "ackaratorn          vidhamba            -0.19035\n",
      "lanús               garlicka            -0.16384\n",
      "leachable           jacovleff           -0.13062\n",
      "\n",
      "Within 1000 word pairs, the most similar and unsimilar words:\n",
      "words\t\t\t\t\tdistance\n",
      "pura                lunaspis             0.75509\n",
      "kochuri             htc                  0.72150\n",
      "streamlining        kandinsky            0.69936\n",
      "venus               sy                   0.74617\n",
      "hezhong             gadot                0.69031\n",
      "approvingalthough   overuse             -0.28227\n",
      "kuosheng            cocea               -0.34014\n",
      "hasim               kerber              -0.29450\n",
      "wykoff              cuitláhuac          -0.30293\n",
      "ballaghadereen      mcdonough           -0.36901\n",
      "\n",
      "Within 10000 word pairs, the most similar and unsimilar words:\n",
      "words\t\t\t\t\tdistance\n",
      "andersson           wonka                0.84971\n",
      "mykola              aníbal               0.80589\n",
      "gheorghiu           dürer                0.82447\n",
      "borg                karras               0.91390\n",
      "oorani              blofeld              0.81263\n",
      "hcesar              monopolisation      -0.37454\n",
      "seghal              perkins             -0.41082\n",
      "livezeanu           videography         -0.35250\n",
      "malabita            pyrogenic           -0.32641\n",
      "interlinks          aldhelm             -0.43275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num in (100, 1000, 10000):\n",
    "    word_similar = []\n",
    "    word_unsimilar = []\n",
    "    dist_similar = []\n",
    "    dist_unsimilar = []\n",
    "    \n",
    "    for _ in range(num):\n",
    "        word_sample = random.sample(words, 2)\n",
    "        word_vector = [word2vec.wv[word_sample[0]], word2vec.wv[word_sample[1]]]\n",
    "        dist = np.dot(word_vector[0], word_vector[1]) / (LA.norm(word_vector[0]) * LA.norm(word_vector[1]))\n",
    "            \n",
    "        if len(dist_similar) < 5:\n",
    "            word_similar.append([word_sample[0], word_sample[1]])\n",
    "            dist_similar.append(dist)\n",
    "        elif dist > min(dist_similar):\n",
    "            index = dist_similar.index(min(dist_similar))\n",
    "            word_similar[index] = [word_sample[0], word_sample[1]]\n",
    "            dist_similar[index] = dist\n",
    "            \n",
    "        if len(dist_unsimilar) < 5:\n",
    "            word_unsimilar.append([word_sample[0], word_sample[1]])\n",
    "            dist_unsimilar.append(dist)\n",
    "        elif dist < max(dist_unsimilar):\n",
    "            index = dist_unsimilar.index(max(dist_unsimilar))\n",
    "            word_unsimilar[index] = [word_sample[0], word_sample[1]]\n",
    "            dist_unsimilar[index] = dist\n",
    "    \n",
    "    print('Within', num, 'word pairs, the most similar and unsimilar words:')\n",
    "    print('words\\t\\t\\t\\t\\tdistance')\n",
    "    for index in range(5):\n",
    "        print('%-20s%-20s%8.5f' % (word_similar[index][0], word_similar[index][1], dist_similar[index]))\n",
    "    for index in range(5):\n",
    "        print('%-20s%-20s%8.5f' % (word_unsimilar[index][0], word_unsimilar[index][1], dist_unsimilar[index]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1257bb",
   "metadata": {},
   "source": [
    "可以看到，\n",
    "\n",
    "## 3. 文档编码\n",
    "\n",
    "分别使用词汇编码均值、第一段词汇编码均值、文档编码算法三种方法进行文档编码，\n",
    "\n",
    "接着使用k均值聚类算法、计算微观和宏观F1值评价各方法，并使用t-SNE进行可视化。\n",
    "\n",
    "### 3.1 使用词汇编码均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728bbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47e4df82",
   "metadata": {},
   "source": [
    "### 3.2 使用第一段词汇编码均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58242084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e19c650f",
   "metadata": {},
   "source": [
    "### 3.3 使用文档编码算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493c984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
